### Idea
#### Projection onto a vector
![[Screenshot 2024-12-26 at 11.13.00 am.png]]
#### Uniqueness
![[Screenshot 2024-12-26 at 11.14.50 am.png]]
#### Alternative
![[Screenshot 2024-12-26 at 11.17.36 am 1.png]]
#### Projection onto a matrix
![[Pasted image 20241226112713.png]]
Notice this projection to vector is a special case, treating inverse of the matrix as the inverse of a number.
![[Screenshot 2024-12-26 at 11.30.04 am.png]]
### Proof
![[Screenshot 2024-12-26 at 11.31.51 am.png]]
For step 2, the [[Span]] of the column of A = $\mathbf{a_1}\lambda_1 + \mathbf{a_2}\lambda_2 + \dots+\mathbf{a_n}\lambda_n$
if we set $\lambda_1$ to 1 and rest to 0, we see that the residual vector dot with it is 0, this can apply to all with an identity matrix. 
![[Screenshot 2024-12-26 at 11.33.49 am.png]]
We transposed so that the dot product all equals zero, matrix multiplication.
![[Screenshot 2024-12-26 at 11.41.20 am.png]]
![[Screenshot 2024-12-26 at 11.41.39 am.png]]
#### Property 
![[Screenshot 2024-12-26 at 11.42.13 am.png]]
![[Screenshot 2024-12-26 at 12.08.30 pm.png]]
#math #linear_algebra #linear_transformation




